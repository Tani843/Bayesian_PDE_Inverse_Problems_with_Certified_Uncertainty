# Mathematical Background

This document provides the mathematical foundation for Bayesian inverse problems in partial differential equations with certified uncertainty quantification.

## Problem Formulation

### Forward Problem

Consider a general PDE in domain Î© âŠ‚ â„áµˆ:

```
â„’[u; Î¸] = f   in Î©
```

where:
- u(x) is the unknown solution field
- Î¸ âˆˆ Î˜ âŠ‚ â„áµ– are unknown parameters
- â„’ is a differential operator depending on Î¸
- f is the source term

**Common PDE Types:**

1. **Elliptic**: -âˆ‡Â·(D(x,Î¸)âˆ‡u) + R(x,Î¸)u = f(x,Î¸)
2. **Parabolic**: âˆ‚u/âˆ‚t - âˆ‡Â·(D(x,Î¸)âˆ‡u) + R(x,Î¸)u = f(x,Î¸)
3. **Hyperbolic**: âˆ‚Â²u/âˆ‚tÂ² - âˆ‡Â·(D(x,Î¸)âˆ‡u) + R(x,Î¸)u = f(x,Î¸)

### Inverse Problem

Given noisy observations y = [yâ‚, ..., yâ‚™] at locations {xâ‚, ..., xâ‚™}, find Î¸ such that:

```
yáµ¢ = G(Î¸)(xáµ¢) + Îµáµ¢,  i = 1, ..., n
```

where:
- G(Î¸): Î˜ â†’ X is the parameter-to-solution map
- Îµáµ¢ ~ N(0, ÏƒÂ²) are independent noise terms

## Bayesian Framework

### Bayes' Theorem

The posterior distribution is:

```
Ï€(Î¸|y) = Ï€(y|Î¸)Ï€(Î¸) / Ï€(y)
```

where:
- Ï€(Î¸) is the prior distribution
- Ï€(y|Î¸) is the likelihood function
- Ï€(y) is the marginal likelihood (evidence)

### Likelihood Function

For Gaussian noise:

```
Ï€(y|Î¸) = (2Ï€ÏƒÂ²)^(-n/2) exp(-1/(2ÏƒÂ²) ||y - G(Î¸)||Â²)
```

Taking logarithms:

```
log Ï€(y|Î¸) = -n/2 log(2Ï€ÏƒÂ²) - 1/(2ÏƒÂ²) Î£áµ¢(yáµ¢ - G(Î¸)(xáµ¢))Â²
```

### Prior Distributions

**Common Prior Choices:**

1. **Log-normal**: Î¸áµ¢ ~ LogNormal(Î¼áµ¢, Ïƒáµ¢Â²) for positive parameters
2. **Gaussian**: Î¸ ~ N(Î¼, Î£) for unbounded parameters  
3. **Gamma**: Î¸áµ¢ ~ Gamma(Î±, Î²) for positive parameters
4. **Uniform**: Î¸áµ¢ ~ Uniform(a, b) for bounded parameters

## Numerical Methods

### Finite Difference Method

**2D Second-Order Centered Differences:**

For the elliptic operator -âˆ‡Â·(Dâˆ‡u):

```
-D(âˆ‚Â²u/âˆ‚xÂ² + âˆ‚Â²u/âˆ‚yÂ²) â‰ˆ -D((uáµ¢â‚Šâ‚,â±¼ - 2uáµ¢,â±¼ + uáµ¢â‚‹â‚,â±¼)/hÂ² + (uáµ¢,â±¼â‚Šâ‚ - 2uáµ¢,â±¼ + uáµ¢,â±¼â‚‹â‚)/hÂ²)
```

This leads to the linear system:
```
Au = f
```

where A is a sparse matrix with 5-point stencil structure.

### Finite Element Method

**Weak Formulation:**

Find u âˆˆ Hâ‚€Â¹(Î©) such that:
```
âˆ«_Î© Dâˆ‡uÂ·âˆ‡v dx + âˆ«_Î© Ruv dx = âˆ«_Î© fv dx  âˆ€v âˆˆ Hâ‚€Â¹(Î©)
```

**Galerkin Discretization:**

Using basis functions {Ï†â±¼}:
```
u_h = Î£â±¼ uâ±¼Ï†â±¼
```

leads to the system:
```
Ku = F
```

where:
- K is the stiffness matrix: Káµ¢â±¼ = âˆ«_Î© Dâˆ‡Ï†áµ¢Â·âˆ‡Ï†â±¼ + RÏ†áµ¢Ï†â±¼ dx
- F is the load vector: Fáµ¢ = âˆ«_Î© fÏ†áµ¢ dx

## MCMC Methods

### Metropolis-Hastings Algorithm

1. **Proposal**: Î¸* ~ q(Î¸*|Î¸â½áµ—â¾)
2. **Acceptance ratio**: Î± = min(1, Ï€(Î¸*|y)q(Î¸â½áµ—â¾|Î¸*) / (Ï€(Î¸â½áµ—â¾|y)q(Î¸*|Î¸â½áµ—â¾)))
3. **Accept/Reject**: Î¸â½áµ—âºÂ¹â¾ = Î¸* with probability Î±, otherwise Î¸â½áµ—âºÂ¹â¾ = Î¸â½áµ—â¾

**Random Walk Proposals:**
```
Î¸* = Î¸â½áµ—â¾ + Îµ,  Îµ ~ N(0, ÏƒÂ²I)
```

### Hamiltonian Monte Carlo

Introduces auxiliary momentum variables p:
```
H(Î¸, p) = U(Î¸) + K(p) = -log Ï€(Î¸|y) + Â½p^T Mâ»Â¹p
```

Hamilton's equations:
```
dÎ¸/dt = âˆ‚H/âˆ‚p = Mâ»Â¹p
dp/dt = -âˆ‚H/âˆ‚Î¸ = -âˆ‡U(Î¸)
```

## Variational Inference

### Mean-Field Approximation

Approximate the posterior with a factorized distribution:
```
q(Î¸) = âˆáµ¢ qáµ¢(Î¸áµ¢)
```

### Evidence Lower Bound (ELBO)

Maximize:
```
â„’(q) = ğ”¼_q[log Ï€(Î¸, y)] - ğ”¼_q[log q(Î¸)]
      = ğ”¼_q[log Ï€(Î¸|y)] + log Ï€(y) - ğ”¼_q[log q(Î¸)]
```

### Gaussian Variational Families

**Mean-field Gaussian**: q(Î¸) = âˆáµ¢ N(Î¸áµ¢; Î¼áµ¢, Ïƒáµ¢Â²)

**Full-rank Gaussian**: q(Î¸) = N(Î¸; Î¼, Î£)

## Uncertainty Quantification

### Concentration Inequalities

#### Hoeffding's Inequality

For bounded random variables Xáµ¢ âˆˆ [aáµ¢, báµ¢]:
```
â„™(|SÌ„â‚™ - ğ”¼[SÌ„â‚™]| â‰¥ t) â‰¤ 2exp(-2nÂ²tÂ² / Î£áµ¢(báµ¢ - aáµ¢)Â²)
```

#### Bernstein's Inequality

For sub-exponential random variables:
```
â„™(|SÌ„â‚™ - ğ”¼[SÌ„â‚™]| â‰¥ t) â‰¤ 2exp(-ntÂ² / (2ÏƒÂ² + 2bt/3))
```

#### McDiarmid's Inequality

For functions with bounded differences:
```
â„™(|f(Xâ‚,...,Xâ‚™) - ğ”¼[f(Xâ‚,...,Xâ‚™)]| â‰¥ t) â‰¤ 2exp(-2tÂ² / Î£áµ¢cáµ¢Â²)
```

### PAC-Bayes Bounds

#### McAllester Bound

For any posterior Q and prior P:
```
R(Q) â‰¤ RÌ‚(Q) + âˆš((KL(Q||P) + log(4n/Î´)) / (2n))
```

where:
- R(Q) is the true risk
- RÌ‚(Q) is the empirical risk
- KL(Q||P) is the KL divergence

#### Seeger Bound

```
R(Q) â‰¤ RÌ‚(Q) + âˆš((KL(Q||P) + log(2âˆšn/Î´)) / (2n-1))
```

#### Catoni Bound

For bounded losses in [0,1]:
```
R(Q) â‰¤ (1/Î») log ğ”¼_Q[exp(Î»(RÌ‚(Î¸) + âˆš((KL(Q||P) + log(1/Î´)) / n)))]
```

## Convergence Theory

### MCMC Convergence

#### Ergodicity Conditions

1. **Irreducibility**: Can reach any set with positive probability
2. **Aperiodicity**: No cyclical behavior
3. **Positive Recurrence**: Expected return time is finite

#### Central Limit Theorem

For ergodic chains:
```
âˆšn(Î¸Ì„â‚™ - ğ”¼[Î¸]) â†’áµˆ N(0, Î£)
```

where Î£ depends on the autocorrelation structure.

### VI Convergence

#### ELBO Convergence

Under mild conditions:
```
lim_{nâ†’âˆ} â„’â‚™(q*) = â„’(q*)
```

where q* minimizes KL(q||Ï€).

## Error Analysis

### Discretization Error

**Finite Differences**: O(hÂ²) for second-order schemes
**Finite Elements**: O(háµ–âºÂ¹) for degree p elements

### Statistical Error

**MCMC**: O(1/âˆšn) for sample mean estimates
**VI**: Depends on approximation quality and optimization

### Total Error Decomposition

```
||Î¸ - Î¸Ì‚|| â‰¤ ||Î¸ - Î¸â‚•|| + ||Î¸â‚• - Î¸Ì‚â‚•|| + ||Î¸Ì‚â‚• - Î¸Ì‚||
```

where:
- ||Î¸ - Î¸â‚•||: Discretization error
- ||Î¸â‚• - Î¸Ì‚â‚•||: Statistical error
- ||Î¸Ì‚â‚• - Î¸Ì‚||: Computational error

## Advanced Topics

### Multilevel Methods

Use hierarchy of discretizations to reduce computational cost:
```
ğ”¼[Q] â‰ˆ Qâ‚€ + Î£â‚—â‚Œâ‚á´¸ (Qâ‚— - Qâ‚—â‚‹â‚)
```

### Dimension Reduction

**Proper Orthogonal Decomposition (POD)**:
Find basis {Ï†áµ¢} minimizing:
```
Î£â±¼||uâ½Ê²â¾ - Î£áµ¢â‚Œâ‚Ê³ aáµ¢â½Ê²â¾Ï†áµ¢||Â²
```

### Adaptive Methods

**Error Indicators**: Use a posteriori estimates to guide refinement
**Goal-Oriented**: Minimize error in quantities of interest

## Implementation Considerations

### Computational Complexity

- **Forward solve**: O(NÂ³) direct, O(N log N) with multigrid
- **MCMC**: O(M Ã— cost_per_sample) where M is chain length
- **VI**: O(K Ã— N_samples) where K is iterations

### Numerical Stability

- Use appropriate preconditioning
- Monitor condition numbers
- Implement adaptive timestepping for parabolic PDEs

### Parallel Computing

- Domain decomposition for PDE solves
- Multiple chains for MCMC
- Vectorized operations for VI

This mathematical framework provides the theoretical foundation for understanding and implementing the Bayesian PDE inverse problem solver with certified uncertainty quantification.