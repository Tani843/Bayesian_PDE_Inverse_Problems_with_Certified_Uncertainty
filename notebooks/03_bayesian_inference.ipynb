{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference for PDE Inverse Problems\n",
    "\n",
    "This notebook explores Bayesian inference methods for PDE inverse problems:\n",
    "\n",
    "- **MCMC Sampling**: Metropolis-Hastings and Hamiltonian Monte Carlo\n",
    "- **Variational Inference**: Fast approximate inference\n",
    "- **Convergence Diagnostics**: Ensuring reliable results\n",
    "- **Posterior Analysis**: Understanding parameter relationships\n",
    "- **Method Comparison**: When to use which approach\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üîó Bayesian Inference Demo - Setup Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup: Parameter Estimation for Heat Equation\n",
    "\n",
    "We'll work with the 2D heat equation:\n",
    "$$-\\nabla \\cdot (\\kappa \\nabla u) = f(x,y) \\quad \\text{in } \\Omega$$\n",
    "$$u = 0 \\quad \\text{on } \\partial\\Omega$$\n",
    "\n",
    "**Goal**: Estimate unknown parameters $\\theta = [\\kappa, \\sigma]$ where:\n",
    "- $\\kappa$: thermal conductivity \n",
    "- $\\sigma$: source strength parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import or implement framework components\n",
    "try:\n",
    "    from bayesian_pde_solver.pde_solvers import FiniteDifferenceSolver\n",
    "    from bayesian_pde_solver.bayesian_inference import MCMCSampler, VariationalInference\n",
    "    print(\"‚úÖ Using framework implementations\")\n",
    "    framework_available = True\n",
    "except ImportError:\n",
    "    print(\"üìù Using custom implementations\")\n",
    "    framework_available = False\n",
    "    \n",
    "    # Minimal PDE solver implementation\n",
    "    class SimplePDESolver:\n",
    "        def __init__(self, domain_bounds, mesh_size):\n",
    "            self.x_min, self.x_max, self.y_min, self.y_max = domain_bounds\n",
    "            self.nx, self.ny = mesh_size\n",
    "            \n",
    "            self.x = np.linspace(self.x_min, self.x_max, self.nx)\n",
    "            self.y = np.linspace(self.y_min, self.y_max, self.ny)\n",
    "            self.X, self.Y = np.meshgrid(self.x, self.y, indexing='ij')\n",
    "            \n",
    "            self.dx = self.x[1] - self.x[0]\n",
    "            self.dy = self.y[1] - self.y[0]\n",
    "            \n",
    "        def solve(self, kappa, sigma):\n",
    "            \"\"\"Solve 2D Poisson equation using Jacobi iteration.\"\"\"\n",
    "            u = np.zeros((self.nx, self.ny))\n",
    "            \n",
    "            # Source function: Gaussian centered at (0.3, 0.7)\n",
    "            source = sigma * np.exp(-((self.X - 0.3)**2 + (self.Y - 0.7)**2) / 0.05)\n",
    "            \n",
    "            # Jacobi iteration\n",
    "            for iteration in range(1000):\n",
    "                u_new = u.copy()\n",
    "                \n",
    "                for i in range(1, self.nx-1):\n",
    "                    for j in range(1, self.ny-1):\n",
    "                        laplacian = ((u[i+1,j] + u[i-1,j]) / self.dx**2 + \n",
    "                                   (u[i,j+1] + u[i,j-1]) / self.dy**2)\n",
    "                        \n",
    "                        u_new[i,j] = (kappa * laplacian + source[i,j]) / (2*kappa*(1/self.dx**2 + 1/self.dy**2))\n",
    "                \n",
    "                # Boundary conditions (u = 0)\n",
    "                u_new[0, :] = 0\n",
    "                u_new[-1, :] = 0\n",
    "                u_new[:, 0] = 0\n",
    "                u_new[:, -1] = 0\n",
    "                \n",
    "                u = u_new\n",
    "            \n",
    "            return u.ravel()\n",
    "        \n",
    "        def get_coordinates(self):\n",
    "            return np.column_stack([self.X.ravel(), self.Y.ravel()])\n",
    "\n",
    "# Initialize solver\n",
    "solver = SimplePDESolver((0, 1, 0, 1), (31, 31))\n",
    "coordinates = solver.get_coordinates()\n",
    "\n",
    "print(f\"üîß Solver initialized: {solver.nx}√ó{solver.ny} mesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic observation data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters (unknown to inference)\n",
    "kappa_true = 1.5\n",
    "sigma_true = 3.0\n",
    "theta_true = np.array([kappa_true, sigma_true])\n",
    "\n",
    "print(f\"üéØ True parameters: Œ∫ = {kappa_true}, œÉ = {sigma_true}\")\n",
    "\n",
    "# Solve with true parameters\n",
    "u_true = solver.solve(kappa_true, sigma_true)\n",
    "\n",
    "# Observation points (sparse measurements)\n",
    "n_obs = 25\n",
    "obs_indices = np.random.choice(len(u_true), n_obs, replace=False)\n",
    "obs_coords = coordinates[obs_indices]\n",
    "u_obs_true = u_true[obs_indices]\n",
    "\n",
    "# Add measurement noise\n",
    "noise_std = 0.05 * np.max(u_obs_true)\n",
    "noise = np.random.normal(0, noise_std, n_obs)\n",
    "u_obs_noisy = u_obs_true + noise\n",
    "\n",
    "print(f\"üìä Generated {n_obs} observations with {noise_std:.4f} noise level\")\n",
    "\n",
    "# Visualize the problem setup\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# True solution field\n",
    "U_true = u_true.reshape((solver.nx, solver.ny))\n",
    "im1 = axes[0].contourf(solver.X, solver.Y, U_true, levels=20, cmap='viridis')\n",
    "axes[0].scatter(obs_coords[:, 0], obs_coords[:, 1], c='red', s=50, marker='o', \n",
    "               label=f'{n_obs} observations')\n",
    "axes[0].set_title('True Solution Field')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].legend()\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Observations with noise\n",
    "axes[1].scatter(obs_coords[:, 0], obs_coords[:, 1], c=u_obs_true, s=100, \n",
    "               cmap='viridis', marker='o', label='True values', alpha=0.7)\n",
    "axes[1].scatter(obs_coords[:, 0], obs_coords[:, 1], c=u_obs_noisy, s=50, \n",
    "               cmap='viridis', marker='x', label='Noisy observations')\n",
    "axes[1].set_title('Observation Data')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Setup: Prior and Posterior\n",
    "\n",
    "### Prior Specification\n",
    "We use log-normal priors for positive parameters:\n",
    "- $\\kappa \\sim \\text{LogNormal}(0, 1)$\n",
    "- $\\sigma \\sim \\text{LogNormal}(1, 0.5)$\n",
    "\n",
    "### Likelihood Model\n",
    "$$p(\\mathbf{y}|\\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_{noise}^2}\\|\\mathbf{y} - \\mathbf{g}(\\theta)\\|^2\\right)$$\n",
    "\n",
    "where $\\mathbf{g}(\\theta)$ is the PDE solution at observation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bayesian components\n",
    "def log_prior(theta):\n",
    "    \"\"\"Log prior probability.\"\"\"\n",
    "    kappa, sigma = theta\n",
    "    \n",
    "    if kappa <= 0 or sigma <= 0:\n",
    "        return -np.inf\n",
    "    \n",
    "    # Log-normal priors\n",
    "    log_p_kappa = stats.lognorm.logpdf(kappa, s=1.0, scale=1.0)\n",
    "    log_p_sigma = stats.lognorm.logpdf(sigma, s=0.5, scale=np.exp(1.0))\n",
    "    \n",
    "    return log_p_kappa + log_p_sigma\n",
    "\n",
    "def log_likelihood(theta, observations, obs_indices, noise_std):\n",
    "    \"\"\"Log likelihood of observations.\"\"\"\n",
    "    kappa, sigma = theta\n",
    "    \n",
    "    try:\n",
    "        # Solve PDE\n",
    "        solution = solver.solve(kappa, sigma)\n",
    "        predictions = solution[obs_indices]\n",
    "        \n",
    "        # Gaussian likelihood\n",
    "        residuals = observations - predictions\n",
    "        log_lik = -0.5 * np.sum(residuals**2) / noise_std**2\n",
    "        log_lik -= 0.5 * len(observations) * np.log(2 * np.pi * noise_std**2)\n",
    "        \n",
    "        return log_lik\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PDE solve failed: {e}\")\n",
    "        return -np.inf\n",
    "\n",
    "def log_posterior(theta):\n",
    "    \"\"\"Log posterior probability.\"\"\"\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    \n",
    "    ll = log_likelihood(theta, u_obs_noisy, obs_indices, noise_std)\n",
    "    return lp + ll\n",
    "\n",
    "# Test the posterior function\n",
    "test_theta = np.array([1.0, 2.0])\n",
    "test_lp = log_posterior(test_theta)\n",
    "print(f\"‚úÖ Posterior function working: log p(Œ∏) = {test_lp:.3f}\")\n",
    "\n",
    "# Find MAP estimate for initialization\n",
    "def neg_log_posterior(theta):\n",
    "    return -log_posterior(theta)\n",
    "\n",
    "print(\"üîç Finding MAP estimate...\")\n",
    "map_result = minimize(neg_log_posterior, x0=np.array([1.0, 2.0]), \n",
    "                     bounds=[(0.1, 10.0), (0.1, 10.0)],\n",
    "                     method='L-BFGS-B')\n",
    "\n",
    "theta_map = map_result.x\n",
    "print(f\"üìç MAP estimate: Œ∫ = {theta_map[0]:.3f}, œÉ = {theta_map[1]:.3f}\")\n",
    "print(f\"üéØ True values:  Œ∫ = {kappa_true:.3f}, œÉ = {sigma_true:.3f}\")\n",
    "print(f\"üìä MAP log posterior: {-map_result.fun:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Sampling: Metropolis-Hastings Algorithm\n",
    "\n",
    "The Metropolis-Hastings algorithm generates samples from the posterior distribution:\n",
    "\n",
    "1. **Propose**: $\\theta' \\sim q(\\theta'|\\theta^{(t)})$\n",
    "2. **Accept/Reject**: Accept with probability $\\min(1, \\alpha)$ where\n",
    "   $$\\alpha = \\frac{p(\\theta'|\\mathbf{y})q(\\theta^{(t)}|\\theta')}{p(\\theta^{(t)}|\\mathbf{y})q(\\theta'|\\theta^{(t)})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Metropolis-Hastings sampler\n",
    "class MetropolisHastingsSampler:\n",
    "    def __init__(self, log_posterior_fn, initial_state, step_size=0.1):\n",
    "        self.log_posterior_fn = log_posterior_fn\n",
    "        self.current_state = np.array(initial_state)\n",
    "        self.step_size = step_size\n",
    "        self.current_log_prob = log_posterior_fn(self.current_state)\n",
    "        \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Run MCMC sampling.\"\"\"\n",
    "        samples = np.zeros((n_samples, len(self.current_state)))\n",
    "        log_probs = np.zeros(n_samples)\n",
    "        n_accepted = 0\n",
    "        \n",
    "        print(f\"üîó Running MCMC: {n_samples} samples...\")\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Propose new state (random walk)\n",
    "            proposal = self.current_state + np.random.normal(0, self.step_size, len(self.current_state))\n",
    "            \n",
    "            # Compute acceptance probability\n",
    "            proposed_log_prob = self.log_posterior_fn(proposal)\n",
    "            \n",
    "            if np.isfinite(proposed_log_prob):\n",
    "                log_alpha = proposed_log_prob - self.current_log_prob\n",
    "                alpha = min(1.0, np.exp(log_alpha))\n",
    "                \n",
    "                # Accept or reject\n",
    "                if np.random.rand() < alpha:\n",
    "                    self.current_state = proposal\n",
    "                    self.current_log_prob = proposed_log_prob\n",
    "                    n_accepted += 1\n",
    "            \n",
    "            samples[i] = self.current_state\n",
    "            log_probs[i] = self.current_log_prob\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % (n_samples // 10) == 0:\n",
    "                acceptance_rate = n_accepted / (i + 1)\n",
    "                print(f\"   {i+1:5d}/{n_samples} samples, acceptance rate: {acceptance_rate:.3f}\")\n",
    "        \n",
    "        acceptance_rate = n_accepted / n_samples\n",
    "        print(f\"‚úÖ MCMC complete! Final acceptance rate: {acceptance_rate:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'samples': samples,\n",
    "            'log_probs': log_probs,\n",
    "            'acceptance_rate': acceptance_rate\n",
    "        }\n",
    "\n",
    "# Run MCMC sampling\n",
    "if framework_available:\n",
    "    mcmc = MCMCSampler(log_posterior, parameter_dim=2)\n",
    "    mcmc_result = mcmc.sample(n_samples=5000, initial_state=theta_map, step_size=0.05)\n",
    "else:\n",
    "    mcmc = MetropolisHastingsSampler(log_posterior, theta_map, step_size=0.05)\n",
    "    mcmc_result = mcmc.sample(n_samples=5000)\n",
    "\n",
    "mcmc_samples = mcmc_result['samples']\n",
    "mcmc_acceptance = mcmc_result['acceptance_rate']\n",
    "\n",
    "print(f\"\\nüìä MCMC Results Summary:\")\n",
    "print(f\"   Samples collected: {len(mcmc_samples)}\")\n",
    "print(f\"   Acceptance rate: {mcmc_acceptance:.3f}\")\n",
    "print(f\"   Target range: 0.2 - 0.6 (optimal: ~0.4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Diagnostics and Visualization\n",
    "def plot_mcmc_traces(samples, parameter_names, true_values=None, burnin=500):\n",
    "    \"\"\"Plot MCMC traces and marginal distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    colors = ['blue', 'red']\n",
    "    \n",
    "    for i, (param_name, color) in enumerate(zip(parameter_names, colors)):\n",
    "        # Trace plot\n",
    "        axes[0, i].plot(samples[:, i], color=color, alpha=0.7, linewidth=0.8)\n",
    "        axes[0, i].axvline(burnin, color='black', linestyle='--', alpha=0.5, label='Burn-in')\n",
    "        if true_values is not None:\n",
    "            axes[0, i].axhline(true_values[i], color='green', linestyle='-', linewidth=2, label='True value')\n",
    "        axes[0, i].set_title(f'Trace: {param_name}')\n",
    "        axes[0, i].set_xlabel('Iteration')\n",
    "        axes[0, i].set_ylabel(param_name)\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Marginal distribution\n",
    "        post_burnin = samples[burnin:, i]\n",
    "        axes[1, i].hist(post_burnin, bins=50, density=True, alpha=0.7, color=color, \n",
    "                       label=f'Posterior (n={len(post_burnin)})')\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_val = np.mean(post_burnin)\n",
    "        std_val = np.std(post_burnin)\n",
    "        axes[1, i].axvline(mean_val, color='black', linestyle='-', linewidth=2, \n",
    "                          label=f'Mean: {mean_val:.3f}')\n",
    "        axes[1, i].axvline(mean_val - std_val, color='black', linestyle='--', alpha=0.7)\n",
    "        axes[1, i].axvline(mean_val + std_val, color='black', linestyle='--', alpha=0.7, \n",
    "                          label=f'¬±1œÉ: {std_val:.3f}')\n",
    "        \n",
    "        if true_values is not None:\n",
    "            axes[1, i].axvline(true_values[i], color='green', linestyle='-', linewidth=2, \n",
    "                             label='True value')\n",
    "        \n",
    "        axes[1, i].set_title(f'Marginal: {param_name}')\n",
    "        axes[1, i].set_xlabel(param_name)\n",
    "        axes[1, i].set_ylabel('Density')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot MCMC results\n",
    "parameter_names = ['Œ∫ (conductivity)', 'œÉ (source strength)']\n",
    "fig = plot_mcmc_traces(mcmc_samples, parameter_names, true_values=theta_true, burnin=500)\n",
    "plt.show()\n",
    "\n",
    "# Compute posterior statistics\n",
    "burnin = 500\n",
    "post_samples = mcmc_samples[burnin:]\n",
    "\n",
    "posterior_means = np.mean(post_samples, axis=0)\n",
    "posterior_stds = np.std(post_samples, axis=0)\n",
    "posterior_quantiles = np.percentile(post_samples, [2.5, 97.5], axis=0)\n",
    "\n",
    "print(\"\\nüìà Posterior Statistics (after burn-in):\")\n",
    "print(\"=\"*60)\n",
    "for i, name in enumerate(['Œ∫', 'œÉ']):\n",
    "    mean_val = posterior_means[i]\n",
    "    std_val = posterior_stds[i]\n",
    "    ci_low, ci_high = posterior_quantiles[:, i]\n",
    "    true_val = theta_true[i]\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Posterior mean: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
    "    print(f\"      95% credible interval: [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    print(f\"      True value: {true_val:.3f} {'‚úì' if ci_low <= true_val <= ci_high else '‚úó'}\")\n",
    "    print(f\"      Relative error: {abs(mean_val - true_val)/true_val*100:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "Essential for ensuring MCMC reliability:\n",
    "- **Effective Sample Size (ESS)**: Independent samples after autocorrelation\n",
    "- **Autocorrelation Function**: How quickly chains mix\n",
    "- **Gelman-Rubin Statistic**: Comparing multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence diagnostics\n",
    "def autocorrelation_function(x, max_lag=200):\n",
    "    \"\"\"Compute autocorrelation function.\"\"\"\n",
    "    n = len(x)\n",
    "    x_centered = x - np.mean(x)\n",
    "    \n",
    "    # Use FFT for efficiency\n",
    "    f_x = np.fft.fft(x_centered, 2*n-1)\n",
    "    autocorr_fft = np.fft.ifft(f_x * np.conj(f_x)).real\n",
    "    autocorr = autocorr_fft[:n] / autocorr_fft[0]\n",
    "    \n",
    "    return autocorr[:min(max_lag, n)]\n",
    "\n",
    "def effective_sample_size(x):\n",
    "    \"\"\"Compute effective sample size.\"\"\"\n",
    "    autocorr = autocorrelation_function(x)\n",
    "    \n",
    "    # Find integrated autocorrelation time\n",
    "    # Sum until autocorrelation becomes negative or very small\n",
    "    tau_int = 1.0\n",
    "    for i in range(1, len(autocorr)):\n",
    "        if autocorr[i] <= 0.01:  # Cutoff at 1%\n",
    "            break\n",
    "        tau_int += 2 * autocorr[i]\n",
    "    \n",
    "    return len(x) / tau_int\n",
    "\n",
    "# Analyze convergence\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, param_name in enumerate(parameter_names):\n",
    "    param_samples = post_samples[:, i]\n",
    "    \n",
    "    # Autocorrelation\n",
    "    autocorr = autocorrelation_function(param_samples, max_lag=200)\n",
    "    lags = np.arange(len(autocorr))\n",
    "    \n",
    "    axes[0, i].plot(lags, autocorr, 'b-', linewidth=2)\n",
    "    axes[0, i].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[0, i].axhline(0.01, color='red', linestyle='--', alpha=0.5, label='1% threshold')\n",
    "    axes[0, i].set_xlabel('Lag')\n",
    "    axes[0, i].set_ylabel('Autocorrelation')\n",
    "    axes[0, i].set_title(f'Autocorrelation: {param_name}')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Running average\n",
    "    running_mean = np.cumsum(param_samples) / np.arange(1, len(param_samples) + 1)\n",
    "    axes[1, i].plot(running_mean, 'b-', linewidth=2, label='Running mean')\n",
    "    axes[1, i].axhline(theta_true[i], color='green', linestyle='-', linewidth=2, label='True value')\n",
    "    axes[1, i].axhline(np.mean(param_samples), color='red', linestyle='--', linewidth=2, label='Final mean')\n",
    "    axes[1, i].set_xlabel('Sample number')\n",
    "    axes[1, i].set_ylabel(param_name)\n",
    "    axes[1, i].set_title(f'Running Mean: {param_name}')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute ESS\n",
    "print(\"üîç Convergence Diagnostics:\")\n",
    "print(\"=\"*50)\n",
    "for i, param_name in enumerate(['Œ∫', 'œÉ']):\n",
    "    ess = effective_sample_size(post_samples[:, i])\n",
    "    print(f\"   {param_name}:\")\n",
    "    print(f\"      Effective sample size: {ess:.1f} / {len(post_samples)} ({ess/len(post_samples)*100:.1f}%)\")\n",
    "    print(f\"      Autocorrelation time: {len(post_samples)/ess:.1f} samples\\n\")\n",
    "\n",
    "total_ess = min([effective_sample_size(post_samples[:, i]) for i in range(2)])\n",
    "print(f\"üí° Minimum ESS: {total_ess:.1f} (rule of thumb: >400 for reliable inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference: Fast Approximate Bayesian Inference\n",
    "\n",
    "When MCMC is too slow, variational inference provides a fast alternative by:\n",
    "1. **Approximating** the posterior with a simpler distribution family\n",
    "2. **Optimizing** the ELBO (Evidence Lower BOund)\n",
    "3. **Trading accuracy for speed**\n",
    "\n",
    "We use mean-field approximation: $q(\\theta) = \\prod_i q_i(\\theta_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement simple variational inference\n",
    "class MeanFieldVI:\n",
    "    def __init__(self, log_posterior_fn, parameter_dim):\n",
    "        self.log_posterior_fn = log_posterior_fn\n",
    "        self.parameter_dim = parameter_dim\n",
    "        \n",
    "        # Variational parameters (mean and log std for each parameter)\n",
    "        self.means = np.zeros(parameter_dim)\n",
    "        self.log_stds = np.log(0.5) * np.ones(parameter_dim)  # Initial std = 0.5\n",
    "        \n",
    "    def sample_variational(self, n_samples):\n",
    "        \"\"\"Sample from variational distribution.\"\"\"\n",
    "        stds = np.exp(self.log_stds)\n",
    "        samples = np.random.normal(self.means, stds, (n_samples, self.parameter_dim))\n",
    "        return samples\n",
    "    \n",
    "    def variational_log_prob(self, samples):\n",
    "        \"\"\"Log probability under variational distribution.\"\"\"\n",
    "        stds = np.exp(self.log_stds)\n",
    "        log_probs = np.sum(stats.norm.logpdf(samples, self.means, stds), axis=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def elbo(self, n_samples=100):\n",
    "        \"\"\"Compute Evidence Lower BOund.\"\"\"\n",
    "        samples = self.sample_variational(n_samples)\n",
    "        \n",
    "        # E_q[log p(Œ∏, y)]\n",
    "        log_joint = np.array([self.log_posterior_fn(sample) for sample in samples])\n",
    "        \n",
    "        # E_q[log q(Œ∏)]\n",
    "        log_q = self.variational_log_prob(samples)\n",
    "        \n",
    "        # ELBO = E_q[log p(Œ∏, y)] - E_q[log q(Œ∏)]\n",
    "        elbo_val = np.mean(log_joint - log_q)\n",
    "        \n",
    "        return elbo_val\n",
    "    \n",
    "    def elbo_gradient(self, n_samples=100, h=1e-5):\n",
    "        \"\"\"Compute ELBO gradient using finite differences.\"\"\"\n",
    "        baseline_elbo = self.elbo(n_samples)\n",
    "        \n",
    "        gradients = np.zeros(2 * self.parameter_dim)  # means + log_stds\n",
    "        \n",
    "        # Gradient w.r.t. means\n",
    "        for i in range(self.parameter_dim):\n",
    "            self.means[i] += h\n",
    "            perturbed_elbo = self.elbo(n_samples)\n",
    "            gradients[i] = (perturbed_elbo - baseline_elbo) / h\n",
    "            self.means[i] -= h\n",
    "        \n",
    "        # Gradient w.r.t. log_stds\n",
    "        for i in range(self.parameter_dim):\n",
    "            self.log_stds[i] += h\n",
    "            perturbed_elbo = self.elbo(n_samples)\n",
    "            gradients[self.parameter_dim + i] = (perturbed_elbo - baseline_elbo) / h\n",
    "            self.log_stds[i] -= h\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def optimize(self, n_iterations=1000, learning_rate=0.01, n_samples=50):\n",
    "        \"\"\"Optimize variational parameters.\"\"\"\n",
    "        elbo_history = []\n",
    "        \n",
    "        print(f\"üîÑ Running variational inference: {n_iterations} iterations...\")\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            # Compute ELBO and gradient\n",
    "            current_elbo = self.elbo(n_samples)\n",
    "            elbo_history.append(current_elbo)\n",
    "            \n",
    "            grad = self.elbo_gradient(n_samples)\n",
    "            \n",
    "            # Gradient ascent update\n",
    "            self.means += learning_rate * grad[:self.parameter_dim]\n",
    "            self.log_stds += learning_rate * grad[self.parameter_dim:]\n",
    "            \n",
    "            # Progress\n",
    "            if (iteration + 1) % (n_iterations // 10) == 0:\n",
    "                print(f\"   Iteration {iteration+1:4d}: ELBO = {current_elbo:.3f}\")\n",
    "        \n",
    "        print(f\"‚úÖ VI optimization complete!\")\n",
    "        \n",
    "        return {\n",
    "            'means': self.means.copy(),\n",
    "            'stds': np.exp(self.log_stds.copy()),\n",
    "            'elbo_history': elbo_history\n",
    "        }\n",
    "\n",
    "# Run variational inference\n",
    "if framework_available:\n",
    "    vi = VariationalInference(log_posterior, parameter_dim=2, vi_type=\"mean_field\")\n",
    "    vi_result = vi.optimize(n_iterations=500, learning_rate=0.01)\n",
    "else:\n",
    "    vi = MeanFieldVI(log_posterior, parameter_dim=2)\n",
    "    # Initialize near MAP\n",
    "    vi.means = theta_map.copy()\n",
    "    vi_result = vi.optimize(n_iterations=500, learning_rate=0.01, n_samples=50)\n",
    "\n",
    "print(f\"\\nüìä VI Results Summary:\")\n",
    "print(f\"   Final ELBO: {vi_result['elbo_history'][-1]:.3f}\")\n",
    "print(f\"   ELBO improvement: {vi_result['elbo_history'][-1] - vi_result['elbo_history'][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare VI and MCMC results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# ELBO convergence\n",
    "axes[0, 0].plot(vi_result['elbo_history'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('ELBO')\n",
    "axes[0, 0].set_title('VI Convergence')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Generate VI samples for comparison\n",
    "vi_samples = vi.sample_variational(len(post_samples))\n",
    "\n",
    "# Parameter comparisons\n",
    "for i, param_name in enumerate(parameter_names):\n",
    "    ax_idx = i + 1\n",
    "    \n",
    "    # Marginal distributions\n",
    "    axes[0, ax_idx].hist(post_samples[:, i], bins=50, density=True, alpha=0.6, \n",
    "                        color='blue', label='MCMC')\n",
    "    axes[0, ax_idx].hist(vi_samples[:, i], bins=50, density=True, alpha=0.6, \n",
    "                        color='red', label='VI')\n",
    "    axes[0, ax_idx].axvline(theta_true[i], color='green', linestyle='-', linewidth=2, \n",
    "                           label='True')\n",
    "    axes[0, ax_idx].set_xlabel(param_name)\n",
    "    axes[0, ax_idx].set_ylabel('Density')\n",
    "    axes[0, ax_idx].set_title(f'Marginal: {param_name}')\n",
    "    axes[0, ax_idx].legend()\n",
    "    axes[0, ax_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Joint distribution scatter plots\n",
    "# MCMC\n",
    "axes[1, 0].scatter(post_samples[::10, 0], post_samples[::10, 1], \n",
    "                  alpha=0.6, s=20, color='blue', label='MCMC')\n",
    "axes[1, 0].scatter(theta_true[0], theta_true[1], \n",
    "                  color='green', s=100, marker='*', label='True')\n",
    "axes[1, 0].set_xlabel('Œ∫ (conductivity)')\n",
    "axes[1, 0].set_ylabel('œÉ (source strength)')\n",
    "axes[1, 0].set_title('MCMC Joint Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# VI\n",
    "axes[1, 1].scatter(vi_samples[::10, 0], vi_samples[::10, 1], \n",
    "                  alpha=0.6, s=20, color='red', label='VI')\n",
    "axes[1, 1].scatter(theta_true[0], theta_true[1], \n",
    "                  color='green', s=100, marker='*', label='True')\n",
    "axes[1, 1].set_xlabel('Œ∫ (conductivity)')\n",
    "axes[1, 1].set_ylabel('œÉ (source strength)')\n",
    "axes[1, 1].set_title('VI Joint Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "axes[1, 2].scatter(post_samples[::20, 0], post_samples[::20, 1], \n",
    "                  alpha=0.4, s=15, color='blue', label='MCMC')\n",
    "axes[1, 2].scatter(vi_samples[::20, 0], vi_samples[::20, 1], \n",
    "                  alpha=0.4, s=15, color='red', label='VI')\n",
    "axes[1, 2].scatter(theta_true[0], theta_true[1], \n",
    "                  color='green', s=100, marker='*', label='True')\n",
    "axes[1, 2].set_xlabel('Œ∫ (conductivity)')\n",
    "axes[1, 2].set_ylabel('œÉ (source strength)')\n",
    "axes[1, 2].set_title('MCMC vs VI Comparison')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantitative comparison\n",
    "vi_means = np.mean(vi_samples, axis=0)\n",
    "vi_stds = np.std(vi_samples, axis=0)\n",
    "vi_quantiles = np.percentile(vi_samples, [2.5, 97.5], axis=0)\n",
    "\n",
    "print(\"\\nüìä Method Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Parameter':<12} {'True':<8} {'MCMC Mean':<12} {'VI Mean':<12} {'MCMC Std':<12} {'VI Std':<12}\")\n",
    "print(\"-\"*80)\n",
    "for i, name in enumerate(['Œ∫', 'œÉ']):\n",
    "    true_val = theta_true[i]\n",
    "    mcmc_mean = posterior_means[i]\n",
    "    mcmc_std = posterior_stds[i]\n",
    "    vi_mean = vi_means[i]\n",
    "    vi_std = vi_stds[i]\n",
    "    \n",
    "    print(f\"{name:<12} {true_val:<8.3f} {mcmc_mean:<12.3f} {vi_mean:<12.3f} {mcmc_std:<12.3f} {vi_std:<12.3f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ MCMC captures true posterior shape and correlations\")\n",
    "print(\"   ‚Ä¢ VI is faster but assumes independence (mean-field)\")\n",
    "print(\"   ‚Ä¢ VI may underestimate uncertainty due to approximation\")\n",
    "print(\"   ‚Ä¢ Choose method based on accuracy vs speed requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced MCMC: Hamiltonian Monte Carlo\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) uses gradient information to make more efficient proposals:\n",
    "- **Leverages gradients** of the log posterior\n",
    "- **Reduces random walk behavior**\n",
    "- **Higher acceptance rates**\n",
    "- **Better exploration** of parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified HMC implementation\n",
    "def finite_difference_gradient(func, x, h=1e-6):\n",
    "    \"\"\"Compute gradient using finite differences.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        x_minus[i] -= h\n",
    "        \n",
    "        grad[i] = (func(x_plus) - func(x_minus)) / (2 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "class SimpleHMC:\n",
    "    def __init__(self, log_posterior_fn, initial_state, step_size=0.01, n_leapfrog=10):\n",
    "        self.log_posterior_fn = log_posterior_fn\n",
    "        self.current_state = np.array(initial_state)\n",
    "        self.step_size = step_size\n",
    "        self.n_leapfrog = n_leapfrog\n",
    "        self.current_log_prob = log_posterior_fn(self.current_state)\n",
    "    \n",
    "    def leapfrog(self, q, p):\n",
    "        \"\"\"Leapfrog integration for Hamiltonian dynamics.\"\"\"\n",
    "        # Half step for momentum\n",
    "        grad = finite_difference_gradient(self.log_posterior_fn, q)\n",
    "        p = p + 0.5 * self.step_size * grad\n",
    "        \n",
    "        # Full steps\n",
    "        for i in range(self.n_leapfrog):\n",
    "            # Full step for position\n",
    "            q = q + self.step_size * p\n",
    "            \n",
    "            # Full step for momentum (except last)\n",
    "            if i < self.n_leapfrog - 1:\n",
    "                grad = finite_difference_gradient(self.log_posterior_fn, q)\n",
    "                p = p + self.step_size * grad\n",
    "        \n",
    "        # Final half step for momentum\n",
    "        grad = finite_difference_gradient(self.log_posterior_fn, q)\n",
    "        p = p + 0.5 * self.step_size * grad\n",
    "        \n",
    "        return q, p\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Run HMC sampling.\"\"\"\n",
    "        samples = np.zeros((n_samples, len(self.current_state)))\n",
    "        log_probs = np.zeros(n_samples)\n",
    "        n_accepted = 0\n",
    "        \n",
    "        print(f\"üöÄ Running HMC: {n_samples} samples...\")\n",
    "        print(f\"   Step size: {self.step_size}, Leapfrog steps: {self.n_leapfrog}\")\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Sample momentum\n",
    "            p = np.random.normal(0, 1, len(self.current_state))\n",
    "            \n",
    "            # Current energy\n",
    "            current_energy = -self.current_log_prob + 0.5 * np.sum(p**2)\n",
    "            \n",
    "            # Leapfrog integration\n",
    "            q_new, p_new = self.leapfrog(self.current_state.copy(), p.copy())\n",
    "            \n",
    "            # Proposed energy\n",
    "            proposed_log_prob = self.log_posterior_fn(q_new)\n",
    "            \n",
    "            if np.isfinite(proposed_log_prob):\n",
    "                proposed_energy = -proposed_log_prob + 0.5 * np.sum(p_new**2)\n",
    "                \n",
    "                # Accept/reject\n",
    "                energy_diff = current_energy - proposed_energy\n",
    "                if energy_diff > 0 or np.random.rand() < np.exp(energy_diff):\n",
    "                    self.current_state = q_new\n",
    "                    self.current_log_prob = proposed_log_prob\n",
    "                    n_accepted += 1\n",
    "            \n",
    "            samples[i] = self.current_state\n",
    "            log_probs[i] = self.current_log_prob\n",
    "            \n",
    "            # Progress\n",
    "            if (i + 1) % (n_samples // 10) == 0:\n",
    "                acceptance_rate = n_accepted / (i + 1)\n",
    "                print(f\"   {i+1:5d}/{n_samples} samples, acceptance rate: {acceptance_rate:.3f}\")\n",
    "        \n",
    "        acceptance_rate = n_accepted / n_samples\n",
    "        print(f\"‚úÖ HMC complete! Final acceptance rate: {acceptance_rate:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'samples': samples,\n",
    "            'log_probs': log_probs,\n",
    "            'acceptance_rate': acceptance_rate\n",
    "        }\n",
    "\n",
    "# Run HMC (smaller sample size due to computational cost)\n",
    "print(\"‚ö° Demonstrating Hamiltonian Monte Carlo...\")\n",
    "hmc = SimpleHMC(log_posterior, theta_map, step_size=0.005, n_leapfrog=20)\n",
    "hmc_result = hmc.sample(n_samples=1000)\n",
    "\n",
    "hmc_samples = hmc_result['samples']\n",
    "hmc_acceptance = hmc_result['acceptance_rate']\n",
    "\n",
    "print(f\"\\nüìä HMC vs MCMC Comparison:\")\n",
    "print(f\"   HMC acceptance rate: {hmc_acceptance:.3f}\")\n",
    "print(f\"   MCMC acceptance rate: {mcmc_acceptance:.3f}\")\n",
    "print(f\"   HMC target range: 0.6 - 0.9 (higher than MCMC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Parameter 1: Œ∫ (conductivity)\n",
    "axes[0, 0].hist(post_samples[:, 0], bins=30, density=True, alpha=0.6, \n",
    "               color='blue', label=f'MCMC (n={len(post_samples)})')\n",
    "axes[0, 0].hist(vi_samples[:, 0], bins=30, density=True, alpha=0.6, \n",
    "               color='red', label=f'VI (n={len(vi_samples)})')\n",
    "axes[0, 0].hist(hmc_samples[100:, 0], bins=20, density=True, alpha=0.6, \n",
    "               color='orange', label=f'HMC (n={len(hmc_samples)-100})')\n",
    "axes[0, 0].axvline(theta_true[0], color='green', linestyle='-', linewidth=2, label='True')\n",
    "axes[0, 0].set_xlabel('Œ∫ (conductivity)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Marginal Distribution: Œ∫')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter 2: œÉ (source strength)\n",
    "axes[0, 1].hist(post_samples[:, 1], bins=30, density=True, alpha=0.6, \n",
    "               color='blue', label=f'MCMC (n={len(post_samples)})')\n",
    "axes[0, 1].hist(vi_samples[:, 1], bins=30, density=True, alpha=0.6, \n",
    "               color='red', label=f'VI (n={len(vi_samples)})')\n",
    "axes[0, 1].hist(hmc_samples[100:, 1], bins=20, density=True, alpha=0.6, \n",
    "               color='orange', label=f'HMC (n={len(hmc_samples)-100})')\n",
    "axes[0, 1].axvline(theta_true[1], color='green', linestyle='-', linewidth=2, label='True')\n",
    "axes[0, 1].set_xlabel('œÉ (source strength)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Marginal Distribution: œÉ')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Joint distributions\n",
    "axes[1, 0].scatter(post_samples[::20, 0], post_samples[::20, 1], \n",
    "                  alpha=0.4, s=15, color='blue', label='MCMC')\n",
    "axes[1, 0].scatter(vi_samples[::20, 0], vi_samples[::20, 1], \n",
    "                  alpha=0.4, s=15, color='red', label='VI')\n",
    "axes[1, 0].scatter(hmc_samples[100::5, 0], hmc_samples[100::5, 1], \n",
    "                  alpha=0.6, s=20, color='orange', label='HMC')\n",
    "axes[1, 0].scatter(theta_true[0], theta_true[1], \n",
    "                  color='green', s=100, marker='*', label='True')\n",
    "axes[1, 0].set_xlabel('Œ∫ (conductivity)')\n",
    "axes[1, 0].set_ylabel('œÉ (source strength)')\n",
    "axes[1, 0].set_title('Joint Distribution Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Effective sample size comparison\n",
    "methods = ['MCMC', 'VI', 'HMC']\n",
    "kappa_ess = [\n",
    "    effective_sample_size(post_samples[:, 0]),\n",
    "    len(vi_samples),  # VI samples are independent\n",
    "    effective_sample_size(hmc_samples[100:, 0])\n",
    "]\n",
    "sigma_ess = [\n",
    "    effective_sample_size(post_samples[:, 1]),\n",
    "    len(vi_samples),  # VI samples are independent\n",
    "    effective_sample_size(hmc_samples[100:, 1])\n",
    "]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, kappa_ess, width, label='Œ∫ ESS', alpha=0.7)\n",
    "axes[1, 1].bar(x + width/2, sigma_ess, width, label='œÉ ESS', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Method')\n",
    "axes[1, 1].set_ylabel('Effective Sample Size')\n",
    "axes[1, 1].set_title('Sampling Efficiency Comparison')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(methods)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comparison table\n",
    "hmc_means = np.mean(hmc_samples[100:], axis=0)\n",
    "hmc_stds = np.std(hmc_samples[100:], axis=0)\n",
    "\n",
    "print(\"\\nüèÜ Final Method Comparison:\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Method':<8} {'Œ∫ Mean':<10} {'Œ∫ Std':<10} {'œÉ Mean':<10} {'œÉ Std':<10} {'Accept Rate':<12} {'Efficiency':<12}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'True':<8} {theta_true[0]:<10.3f} {'-':<10} {theta_true[1]:<10.3f} {'-':<10} {'-':<12} {'-':<12}\")\n",
    "print(f\"{'MCMC':<8} {posterior_means[0]:<10.3f} {posterior_stds[0]:<10.3f} {posterior_means[1]:<10.3f} {posterior_stds[1]:<10.3f} {mcmc_acceptance:<12.3f} {min(kappa_ess[0], sigma_ess[0]):<12.1f}\")\n",
    "print(f\"{'VI':<8} {vi_means[0]:<10.3f} {vi_stds[0]:<10.3f} {vi_means[1]:<10.3f} {vi_stds[1]:<10.3f} {'-':<12} {min(kappa_ess[1], sigma_ess[1]):<12.1f}\")\n",
    "print(f\"{'HMC':<8} {hmc_means[0]:<10.3f} {hmc_stds[0]:<10.3f} {hmc_means[1]:<10.3f} {hmc_stds[1]:<10.3f} {hmc_acceptance:<12.3f} {min(kappa_ess[2], sigma_ess[2]):<12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Selection Guidelines\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "**MCMC (Metropolis-Hastings)**:\n",
    "- ‚úÖ **Gold standard** for accurate posterior sampling\n",
    "- ‚úÖ **Few assumptions** about posterior shape\n",
    "- ‚úÖ **Reliable** for complex, multimodal distributions\n",
    "- ‚ö†Ô∏è **Slow convergence** for high-dimensional problems\n",
    "\n",
    "**Variational Inference**:\n",
    "- ‚úÖ **Fast** - orders of magnitude faster than MCMC\n",
    "- ‚úÖ **Scalable** to high-dimensional problems\n",
    "- ‚úÖ **Deterministic** - reproducible results\n",
    "- ‚ö†Ô∏è **Approximate** - may underestimate uncertainty\n",
    "- ‚ö†Ô∏è **Strong assumptions** (e.g., mean-field independence)\n",
    "\n",
    "**Hamiltonian Monte Carlo**:\n",
    "- ‚úÖ **Efficient exploration** using gradients\n",
    "- ‚úÖ **Higher acceptance rates** than random walk\n",
    "- ‚úÖ **Good for smooth posteriors**\n",
    "- ‚ö†Ô∏è **Requires gradients** (automatic differentiation)\n",
    "- ‚ö†Ô∏è **Tuning required** (step size, integration time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Bayesian Framework**:\n",
    "   - Prior knowledge + Data likelihood ‚Üí Posterior distribution\n",
    "   - Uncertainty quantification is natural and principled\n",
    "   - Parameter correlations are captured\n",
    "\n",
    "2. **MCMC Sampling**:\n",
    "   - Metropolis-Hastings: Simple and robust\n",
    "   - Convergence diagnostics are essential\n",
    "   - Effective sample size matters more than total samples\n",
    "\n",
    "3. **Variational Inference**:\n",
    "   - Trade accuracy for speed\n",
    "   - ELBO optimization provides approximate posterior\n",
    "   - Great for large-scale problems\n",
    "\n",
    "4. **Method Selection**:\n",
    "   - Consider accuracy vs computational budget\n",
    "   - MCMC for small problems requiring high accuracy\n",
    "   - VI for large problems or rapid prototyping\n",
    "   - HMC when gradients are available\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 04**: Certified uncertainty quantification\n",
    "- **Notebook 05**: Visualization gallery\n",
    "- **Experiment**: Try different prior specifications\n",
    "- **Challenge**: Implement your own inference method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create completion summary\n",
    "print(\"üéì Bayesian Inference Demo - Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "skills_learned = [\n",
    "    \"‚úÖ Bayesian inference framework setup\",\n",
    "    \"‚úÖ MCMC sampling (Metropolis-Hastings)\",\n",
    "    \"‚úÖ Convergence diagnostics and ESS\",\n",
    "    \"‚úÖ Variational inference (mean-field)\",\n",
    "    \"‚úÖ Hamiltonian Monte Carlo basics\",\n",
    "    \"‚úÖ Method comparison and selection\",\n",
    "    \"‚úÖ Posterior analysis and interpretation\",\n",
    "    \"‚úÖ Computational efficiency considerations\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Skills Acquired:\")\n",
    "for skill in skills_learned:\n",
    "    print(f\"   {skill}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"üìì Notebook 04: Certified Uncertainty Quantification\",\n",
    "    \"üìì Notebook 05: Visualization Gallery\",\n",
    "    \"üîß Try different priors and likelihoods\",\n",
    "    \"üß™ Experiment with method parameters\",\n",
    "    \"üìä Apply to your own PDE problems\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Bayesian inference provides principled uncertainty quantification,\")\n",
    "print(\"   but method choice depends on accuracy vs efficiency tradeoffs.\")\n",
    "\n",
    "print(\"\\nüéâ Ready for certified bounds? Continue to Notebook 04!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}